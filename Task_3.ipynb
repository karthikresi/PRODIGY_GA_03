{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import defaultdict, Counter\n",
        "import random\n"
      ],
      "metadata": {
        "id": "f3iamp85hctF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/input .txt'"
      ],
      "metadata": {
        "id": "RP1I9K41hj2C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.replace('\\n', ' ')  # Replace newline characters with space\n",
        "    return text"
      ],
      "metadata": {
        "id": "FF-vzcuDiaQD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization (default to word-level, can switch to character-level)\n",
        "def tokenize_text(text, level='word'):\n",
        "    if level == 'word':\n",
        "        return text.split()  # Split text into words\n",
        "    elif level == 'char':\n",
        "        return list(text)  # Split text into characters"
      ],
      "metadata": {
        "id": "moy0zGu5icKQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Markov chain model\n",
        "class MarkovChain:\n",
        "    def __init__(self, order=3):\n",
        "        self.order = order\n",
        "        self.model = defaultdict(Counter)\n",
        "\n",
        "    def add_sequence(self, tokenized_text):\n",
        "        for i in range(len(tokenized_text) - self.order):\n",
        "            current_state = tuple(tokenized_text[i:i + self.order])\n",
        "            next_state = tokenized_text[i + self.order]\n",
        "            self.model[current_state][next_state] += 1\n",
        "\n",
        "    def get_next_token_probability(self, current_state, next_token):\n",
        "        transitions = self.model.get(current_state, Counter())\n",
        "        total = sum(transitions.values())\n",
        "        if total == 0:\n",
        "            return 0.0\n",
        "        return transitions.get(next_token, 0) / total\n",
        "\n",
        "    def generate_sequence(self, seed, length=50):\n",
        "        current_state = seed\n",
        "        result = list(current_state)\n",
        "        for _ in range(length):\n",
        "            transitions = self.model.get(current_state, None)\n",
        "            if not transitions:\n",
        "                break  # Exit loop if current state has no transitions\n",
        "            next_token = random.choices(list(transitions.keys()),\n",
        "                                        list(transitions.values()))[0]\n",
        "            result.append(next_token)\n",
        "            current_state = tuple(result[-self.order:])\n",
        "        return result"
      ],
      "metadata": {
        "id": "qyW3FMJ6ikBb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to execute the entire process\n",
        "def main():\n",
        "    cleaned_text = clean_text(text)\n",
        "    tokenized_text_word = tokenize_text(cleaned_text, level='word')\n",
        "    tokenized_text_char = tokenize_text(cleaned_text, level='char')\n",
        "\n",
        "    # Word-level Markov chain model\n",
        "    markov_model_word = MarkovChain(order=3)\n",
        "    markov_model_word.add_sequence(tokenized_text_word)\n",
        "    seed_word = tuple(tokenized_text_word[:3])  # Use the first three words as the initial state\n",
        "    generated_sequence_word = markov_model_word.generate_sequence(seed_word, length=50)\n",
        "\n",
        "    # Character-level Markov chain model\n",
        "    markov_model_char = MarkovChain(order=4)\n",
        "    markov_model_char.add_sequence(tokenized_text_char)\n",
        "    seed_char = tuple(tokenized_text_char[:4])  # Use the first four characters as the initial state\n",
        "    generated_sequence_char = markov_model_char.generate_sequence(seed_char, length=200)\n",
        "\n",
        "    # Prepare output\n",
        "    output = []\n",
        "    output.append(\"Generated Word Sequence:\")\n",
        "    output.append(' '.join(generated_sequence_word))\n",
        "    output.append(\"\\nGenerated Character Sequence:\")\n",
        "    output.append(''.join(generated_sequence_char))\n",
        "\n",
        "    # Predict next token probabilities for multiple words and characters\n",
        "    word_probabilities = [\n",
        "        (('alice', 'was', 'beginning'), 'to'),\n",
        "        (('to', 'get', 'very'), 'tired'),\n",
        "        (('and', 'of', 'having'), 'nothing'),\n",
        "        (('she', 'had', 'peeped'), 'into'),\n",
        "        (('into', 'the', 'book'), 'her'),\n",
        "        (('book', 'her', 'sister'), 'was'),\n",
        "        (('her', 'sister', 'was'), 'reading'),\n",
        "        (('but', 'it', 'had'), 'no'),\n",
        "        (('no', 'pictures', 'or'), 'conversations'),\n",
        "        (('thought', 'alice', 'without'), 'pictures'),\n",
        "        # Add more (current_state, next_token) tuples as needed\n",
        "    ]\n",
        "\n",
        "    char_probabilities = [\n",
        "        (('a', 'l', 'i', 'c'), 'e'),\n",
        "        (('t', 'h', 'e', ' '), 'b'),\n",
        "        (('o', 'n', 'c', 'e'), ' '),\n",
        "        (('h', 'e', 'r', ' '), 's'),\n",
        "        (('t', 'h', 'o', 'u'), 'g'),\n",
        "        (('r', 'e', 'a', 'd'), 'i'),\n",
        "        (('b', 'o', 'o', 'k'), ' '),\n",
        "        (('c', 'o', 'n', 'v'), 'e'),\n",
        "        (('s', 'i', 's', 't'), 'e'),\n",
        "        (('a', 'n', 'd', ' '), 'o'),\n",
        "        # Add more (current_state, next_token) tuples as needed\n",
        "    ]\n",
        "\n",
        "    for current_state_word, next_token_word in word_probabilities:\n",
        "        next_token_prob_word = markov_model_word.get_next_token_probability(current_state_word, next_token_word)\n",
        "        output.append(f\"\\nProbability of '{next_token_word}' following {current_state_word}: {next_token_prob_word}\")\n",
        "\n",
        "    for current_state_char, next_token_char in char_probabilities:\n",
        "        next_token_prob_char = markov_model_char.get_next_token_probability(current_state_char, next_token_char)\n",
        "        output.append(f\"\\nProbability of '{next_token_char}' following {current_state_char}: {next_token_prob_char}\")\n",
        "\n",
        "    # Write results to output file\n",
        "    with open('result.txt', 'w') as file:\n",
        "        file.write('\\n'.join(output))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Wlw_3Lz7imBz"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}